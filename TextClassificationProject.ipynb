{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keS51tO6pZUQ"
      },
      "source": [
        "# Text classification with RNNs\n",
        "## Preamble: installing and importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 786,
      "metadata": {
        "id": "QBxnnaV-pZUb"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import datasets\n",
        "except ModuleNotFoundError:\n",
        "    !pip install datasets\n",
        "    import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import nltk\n",
        "except ModuleNotFoundError:\n",
        "    !pip install nltk\n",
        "    import nltk"
      ],
      "metadata": {
        "id": "hNzyrZ0ZNIH4"
      },
      "execution_count": 787,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 788,
      "metadata": {
        "id": "m8i6nJI-pZUg"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from unidecode import unidecode\n",
        "except ModuleNotFoundError:\n",
        "    !pip install unidecode\n",
        "    from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 789,
      "metadata": {
        "id": "exNjvu93pZUh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WREwvthzpZUk"
      },
      "source": [
        "## Load training dataset\n",
        "\n",
        "We are going to work with a [ dataset that contains 58k carefully curated Reddit comments labeled for 27 emotions](https://www.tensorflow.org/datasets/catalog/goemotions). \n",
        "This dataset can be retreived using the [`datasets` library from the catalog of tensorflow ](https://huggingface.co/docs/datasets/index).\n",
        "\n",
        "The next cells load some information on the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 34"
      ],
      "metadata": {
        "id": "Av3b10C7q0JA"
      },
      "execution_count": 790,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 791,
      "metadata": {
        "id": "oYzUcydHpZUn"
      },
      "outputs": [],
      "source": [
        "DATA_HANDLE = \"go_emotions\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 792,
      "metadata": {
        "id": "UEmSWsPdpZUo",
        "outputId": "6c6d1a20-7459-471e-9f7a-b44194ae20f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:No config specified, defaulting to: go_emotions/simplified\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset_builder\n",
        "ds_builder = load_dataset_builder(DATA_HANDLE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 793,
      "metadata": {
        "id": "Hw_qlgpnpZUq",
        "outputId": "bfafcd42-3130-453f-ca3b-c8ecb0a59899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The GoEmotions dataset contains 58k carefully curated Reddit comments labeled for 27 emotion categories or Neutral.\\nThe emotion categories are admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire,\\ndisappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness,\\noptimism, pride, realization, relief, remorse, sadness, surprise.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 793
        }
      ],
      "source": [
        "ds_builder.info.description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPvkYMVEpZUs"
      },
      "source": [
        "Each element in the dataset has two features: the review text itself, and the associated label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 794,
      "metadata": {
        "id": "ymYAu3UppZUt",
        "outputId": "722cc111-0379-4daa-9381-a7f2e2fac4b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': Value(dtype='string', id=None),\n",
              " 'labels': Sequence(feature=ClassLabel(names=['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'], id=None), length=-1, id=None),\n",
              " 'id': Value(dtype='string', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 794
        }
      ],
      "source": [
        "ds_builder.info.features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7xf_AkBpZUv"
      },
      "source": [
        "Now we are going to load the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 795,
      "metadata": {
        "id": "RLWutpOJpZUw",
        "outputId": "d1367275-8655-4e22-de4b-428e32d301cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:No config specified, defaulting to: go_emotions/simplified\n",
            "WARNING:datasets.builder:Found cached dataset go_emotions (/root/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d)\n",
            "WARNING:datasets.builder:No config specified, defaulting to: go_emotions/simplified\n",
            "WARNING:datasets.builder:Found cached dataset go_emotions (/root/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d)\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_ds = load_dataset(DATA_HANDLE, split=\"train\")\n",
        "test_ds =  load_dataset(DATA_HANDLE, split=\"test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5c-kVGnpZUx"
      },
      "source": [
        "As seen in `ds_builder.info.features`, each data sample has three fields: the `text` and the `label` string and the id of the text. Here is the text for one particular sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyAiKMhrcZwg",
        "outputId": "235ca982-d8a3-4534-d16e-6f4ae4c70b4d"
      },
      "execution_count": 796,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'labels', 'id'],\n",
              "    num_rows: 43410\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 796
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFK3FczipZUz"
      },
      "source": [
        "### Normalizing characters\n",
        "Some of the tools we'll be using later cannot flawlessly handle all unicode characters. To avoid problems, we will normalize all characters to their closest ASCII equivalent using the function `unidecode` (imported from [`unidecode` package](https://pypi.org/project/Unidecode/)).\n",
        "\n",
        "The function basically replaces all characters bearing [diacritic signs](https://en.wikipedia.org/wiki/Diacritic) with their corresponding plain character, as well as any symbols with close ASCII equivalents. The result is a text with no accents, cedillas, no â‚¬ symbol, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 797,
      "metadata": {
        "id": "Mhw3_WrrpZU0",
        "outputId": "84184327-000a-4f5a-a291-647327c85f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Demographics? I don't know anybody under 35 who has cable tv.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 797
        }
      ],
      "source": [
        "unidecode(train_ds[10]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(lambda sample: {'text': unidecode(sample['text']), 'labels': sample['labels'],'id':sample['id']})"
      ],
      "metadata": {
        "id": "EFwkcN8rc6TU",
        "outputId": "9e352418-1aec-4989-f53a-1c95130afcc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 798,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d/cache-de82329d8d175d89.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n"
      ],
      "metadata": {
        "id": "OnXA2koQICaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD4z5OI0M4UK",
        "outputId": "1bc02619-d687-41f4-d99d-1d628cbe925b"
      },
      "execution_count": 799,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 799
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_categories=['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']"
      ],
      "metadata": {
        "id": "T_142xO8gfeX"
      },
      "execution_count": 800,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(emotion_categories))"
      ],
      "metadata": {
        "id": "d21ajuYn9Sph",
        "outputId": "7242c99e-13a9-42ae-ae98-eaadb1f57a19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 808,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZUMGMkZ09SfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  tokens = [token.lower() for token in tokens if token.isalpha()]\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "  return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "1639a-gHHB9Z"
      },
      "execution_count": 801,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import itertools\n",
        "\n",
        "# Preprocess the data\n",
        "def tokenize(texts):\n",
        "  # Tokenize the texts using the Tokenizer class\n",
        "  tokenizer = Tokenizer(num_words=20000)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  return sequences, tokenizer\n",
        "\n",
        "\n",
        "def encode_labels(labels, num_classes):\n",
        "  # Flatten the list of labels to a single list or array\n",
        "  labels = list(itertools.chain.from_iterable(labels))\n",
        "  # Or use a list comprehension: labels = [label for sublist in labels for label in sublist]\n",
        "  \n",
        "  # One-hot encode the labels\n",
        "  one_hot_labels = to_categorical(labels, num_classes=num_classes)\n",
        "  return one_hot_labels\n",
        "\n",
        "\n",
        "def preprocess_(dataset, max_length):\n",
        "  # Select only the examples with non-empty text and labels\n",
        "  dataset = [example for example in dataset if (example[\"text\"]!=\"\" and example[\"labels\"]!=\"\")]\n",
        "\n",
        "  # Tokenize and encode the text and labels\n",
        "  texts = [example[\"text\"] for example in dataset]\n",
        "  print(f\" text : {len(texts)}\")\n",
        "  sequences, tokenizer = tokenize(texts)\n",
        "  sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "  print(sequences.shape)\n",
        "  labels = [example[\"labels\"] for example in dataset]\n",
        "  print(f\" labels : {len(labels)}\")\n",
        "  one_hot_labels = encode_labels(labels, num_classes=len(emotion_categories))\n",
        "  print(f\" after one hot labels {one_hot_labels.shape}\")\n",
        "  padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "  padded_one_hot_labels = pad_sequences(one_hot_labels, maxlen=max_length)\n",
        "  print(padded_one_hot_labels.shape)\n",
        "  print(padded_sequences.shape)\n",
        "  return padded_sequences, padded_one_hot_labels, tokenizer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_xUTySMdf8Zm"
      },
      "execution_count": 802,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess(dataset, max_length):\n",
        "  # Select only the examples with non-empty text and labels\n",
        "  dataset = [example for example in dataset if example[\"text\"] and example[\"labels\"]]\n",
        "  # Tokenize and encode the text and labels\n",
        "  texts = [example[\"text\"] for example in dataset]\n",
        "  sequences, tokenizer = tokenize(texts)\n",
        "  sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "  labels = [example[\"labels\"] for example in dataset]\n",
        "  one_hot_labels = encode_labels(labels, num_classes=len(emotion_categories))\n",
        "  # Create a TensorFlow dataset from the preprocessed data\n",
        "  tf_dataset = tf.data.Dataset.from_tensor_slices((sequences, one_hot_labels))\n",
        "  # Shuffle and batch the examples\n",
        "  tf_dataset = tf_dataset.shuffle(buffer_size=len(sequences)).batch(BATCH_SIZE)\n",
        "  return tf_dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ek1jtFy34Jqb"
      },
      "execution_count": 803,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 80\n"
      ],
      "metadata": {
        "id": "i-IXB6_VugBM"
      },
      "execution_count": 804,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data into testing sets\n",
        "test_ds = load_dataset(DATA_HANDLE, split=\"test\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FsOwf-TgsYH",
        "outputId": "b84ce3a3-d0a0-4167-ce9d-b3494e46dfc6"
      },
      "execution_count": 805,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:No config specified, defaulting to: go_emotions/simplified\n",
            "WARNING:datasets.builder:Found cached dataset go_emotions (/root/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter --config-dir\n",
        "!echo \"c.NotebookApp.iopub_data_rate_limit = 10000000\" >> /root/.jupyter/jupyter_notebook_config.py\n"
      ],
      "metadata": {
        "id": "z36qqhH81J08",
        "outputId": "de80fb62-9a10-4ea9-f945-7ea270426a4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 806,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.jupyter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "\n",
        "# Get the maximum length of the examples\n",
        "max_length = max([len(example[\"text\"]) for example in train_ds])\n",
        "\n",
        "train_tfds = preprocess(train_ds, max_length)\n"
      ],
      "metadata": {
        "id": "BFiliCl9wdSZ",
        "outputId": "6f2d2ccc-60b1-4e6d-a43a-fbdf7b9ee0d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "execution_count": 807,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-807-dcc98e451a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_tfds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-803-bf18675096a3>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(dataset, max_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mone_hot_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_categories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Create a TensorFlow dataset from the preprocessed data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mtf_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;31m# Shuffle and batch the examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mtf_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    807\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \"\"\"\n\u001b[0;32m--> 809\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m   4561\u001b[0m         tensor_shape.dimension_value(self._tensors[0].get_shape()[0]))\n\u001b[1;32m   4562\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4563\u001b[0;31m       batch_dim.assert_is_compatible_with(\n\u001b[0m\u001b[1;32m   4564\u001b[0m           tensor_shape.Dimension(\n\u001b[1;32m   4565\u001b[0m               tensor_shape.dimension_value(t.get_shape()[0])))\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \"\"\"\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[0m\u001b[1;32m    299\u001b[0m                        (self, other))\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimensions 43410 and 51103 are not compatible"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "max_length = 50  # maximum length of the sequences\n",
        "X_train, y_train, tokenizer = preprocess(train_ds, max_length)\n",
        "X_test, y_test, _ = preprocess(test_ds, max_length)\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(20000, 128, input_length=max_length),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(len(emotion_categories), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "n3iLJpn-gyK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "d2sKMfXWjhIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "xwvS4mgfjloI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "r036dmqNg9R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPTIMIZED MODEL \n"
      ],
      "metadata": {
        "id": "e-6hyIWBTD7j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UnryomGMTHj0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1e6b277c3cccbcac03c402ee034fc45e432e7e3edba369992c2d20da2d6e2721"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}